{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "# Set the path to your data folder\n",
    "data_folder = 'CMPE280_DataSources_20241107_Hackathon'\n",
    "\n",
    "# Function to extract text from CSV files\n",
    "def extract_text_from_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return \" \".join(df.astype(str).values.flatten().tolist())\n",
    "\n",
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(filepath):\n",
    "    text = \"\"\n",
    "    with open(filepath, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text() or \"\"\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "# Extract data from all files in the folder\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    filepath = os.path.join(data_folder, filename)\n",
    "    if filename.endswith('.csv'):\n",
    "        documents.append(extract_text_from_csv(filepath))\n",
    "    elif filename.endswith('.pdf'):\n",
    "        documents.append(extract_text_from_pdf(filepath))\n",
    "\n",
    "# Combine all extracted text into one large string\n",
    "full_text = \" \".join(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Split the Text and Create Embeddings Using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/11\" 200 6680\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/11\" 200 6680\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Split the text into chunks for embedding\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "# Create embeddings for each chunk using HuggingFace\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a FAISS index to store the embeddings for fast similarity search\n",
    "faiss_index = FAISS.from_texts(text_chunks, embeddings)\n",
    "\n",
    "faiss_index.save_local(\"faiss_store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Initialize ChatGroq Using Environment Variables and Set Up the Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/shivavardhineedi/Desktop/semester3/CMPE-280/Hackathon_RAG/rag/lib/python3.9/site-packages/certifi/cacert.pem'\n",
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='/Users/shivavardhineedi/Desktop/semester3/CMPE-280/Hackathon_RAG/rag/lib/python3.9/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from duckduckgo_search import DDGS\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the ChatGroq model using API key from environment variable\n",
    "llm = ChatGroq(api_key=os.getenv('GROQ_API_KEY'), max_tokens=200)  # Set max tokens to limit response length\n",
    "\n",
    "# Create a retriever from the FAISS index\n",
    "retriever = faiss_index.as_retriever()\n",
    "\n",
    "# Configure logging with pretty print\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "pp = pprint\n",
    "\n",
    "# DuckDuckGo search function using DDGS class\n",
    "def duckduckgo_search(query):\n",
    "    logger.info(\"Performing DuckDuckGo search...\")\n",
    "    \n",
    "    ddgs = DDGS()  # Initialize the DuckDuckGo search object\n",
    "    search_results = list(ddgs.text(query, max_results=3))  # Retrieve the top 3 results\n",
    "    \n",
    "    if search_results:\n",
    "        logger.info(\"DuckDuckGo search successful.\")\n",
    "        pp(search_results)  # Pretty print the search results for debugging\n",
    "        # Formatting the results to provide them as context\n",
    "        return \"\\n\".join([f\"{result['title']}: {result['body']} (URL: {result['href']})\" for result in search_results])\n",
    "    else:\n",
    "        logger.warning(\"No relevant results found from DuckDuckGo.\")\n",
    "        return \"No relevant results found from DuckDuckGo.\"\n",
    "\n",
    "# Function to query the system with chunk reduction strategies\n",
    "def query_system(query):\n",
    "    logger.info(f\"Received query: {query}\")\n",
    "\n",
    "    # Retrieve relevant context using the FAISS retriever\n",
    "    relevant_texts = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Limit the number of relevant chunks to reduce context size\n",
    "    MAX_CHUNKS = 5\n",
    "    if len(relevant_texts) > MAX_CHUNKS:\n",
    "        logger.info(f\"Retrieved {len(relevant_texts)} chunks. Limiting to {MAX_CHUNKS} most relevant chunks.\")\n",
    "        relevant_texts = relevant_texts[:MAX_CHUNKS]  # Keep only the top 5 relevant chunks\n",
    "\n",
    "    # Combine the selected chunks into a single context\n",
    "    context = \"\\n\".join([text.page_content for text in relevant_texts])\n",
    "\n",
    "    # If the combined context is still too long, truncate or summarize\n",
    "    MAX_CONTEXT_LENGTH = 2000\n",
    "    if len(context) > MAX_CONTEXT_LENGTH:\n",
    "        logger.info(f\"Context size ({len(context)}) exceeds {MAX_CONTEXT_LENGTH} characters. Truncating context.\")\n",
    "        context = context[:MAX_CONTEXT_LENGTH] + \"... [truncated]\"\n",
    "\n",
    "    # If context is still too small or unavailable, fall back to DuckDuckGo\n",
    "    if len(context.strip()) == 0:\n",
    "        logger.warning(\"No relevant information found in FAISS index.\")\n",
    "        search_results = duckduckgo_search(query)\n",
    "        context = \"No relevant internal data was found. Here are some web search results:\\n\" + search_results\n",
    "        source_info = \"Note: The information provided below is based on a DuckDuckGo web search.\"\n",
    "    else:\n",
    "        logger.info(\"Using information retrieved from the FAISS index.\")\n",
    "        context += \"\\n\\nNote: Additional web information can be provided if needed.\"\n",
    "        source_info = \"Note: The following information was retrieved internally from the FAISS index.\"\n",
    "\n",
    "    # Prepare the messages for ChatGroq invocation with updated prompt for styled and concise responses\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that provides answers in a well-structured, styled format. \"\n",
    "            \"Use bullet points, lists, or sections to make the response easy to read. Keep your response concise, \"\n",
    "            \"Keep the output tokens as less as possible, and only provide the most critical information needed to answer the user's question. \"\n",
    "            \"Avoid excessive details, and focus on clarity and brevity.\\n\\n\"\n",
    "            \"Context:\\n\"\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"{source_info} Please clearly indicate if any part of the answer is based on external web searches.\"\n",
    "        ),\n",
    "        (\"user\", query)\n",
    "    ]\n",
    "\n",
    "    # Invoke the model with the provided messages\n",
    "    logger.info(\"Invoking the ChatGroq model with the provided context and query...\")\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Print debug information about the type of data source used\n",
    "    if \"DuckDuckGo\" in source_info:\n",
    "        logger.info(\"The answer is based on a DuckDuckGo web search.\")\n",
    "    else:\n",
    "        logger.info(\"The answer is based on information retrieved from the FAISS index.\")\n",
    "    \n",
    "    pp(response.content)  # Pretty print the final response for better debugging\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Query the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Received query: What is KMP algorithm?\n",
      "INFO:root:Context size (2942514) exceeds 2000 characters. Truncating context.\n",
      "INFO:root:Using information retrieved from the FAISS index.\n",
      "INFO:root:Invoking the ChatGroq model with the provided context and query...\n",
      "DEBUG:groq._base_client:Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"You are a helpful assistant that provides answers in a well-structured, styled format. Use bullet points, lists, or sections to make the response easy to read. Keep your response concise, Keep the output tokens as less as possible, and only provide the most critical information needed to answer the user's question. Avoid excessive details, and focus on clarity and brevity.\\n\\nContext:\\n1960 nan nan nan 1961 nan nan nan 1962 nan nan nan 1963 nan nan nan 1964 nan nan nan 1965 nan nan nan 1966 nan nan nan 1967 nan nan nan 1968 nan nan nan 1969 nan 0.072826324 0.113667809 1970 nan 0.070763624 0.066102932 1971 nan 0.024893934 0.099287786 1972 nan 0.044331264 0.135402869 1973 nan 0.057241382 0.229090182 1974 nan -0.010486395 0.137099799 1975 nan -0.007502573 0.155331555 1976 nan -0.029682109 0.140261482 1977 nan 0.0131755 0.233883413 1978 4.49e-05 0.03174683 0.306394355 1979 0.029819636 0.042484827 0.585866342 1980 0.135296316 0.047506181 0.800738126 1981 0.209664357 0.03591159 0.634908483 1982 0.275698543 0.002584047 0.316452387 1983 0.483945715 0.009068703 0.624874152 1984 0.536046583 0.045627777 0.221941613 1985 0.623424606 0.047283784 0.675731298 1986 0.847702965 0.076091199 1.302414002 1987 1.022558946 0.030766482 1.086807482 1988 0.97565029 0.085156734 1.343240723 1989 0.966308311 0.073740021 1.194504107 1990 1.138837732 0.02722554 0.561047032 1991 2.613162193 0.095941829 0.464853987 1992 6.186882076 0.197056163 0.732369584 1993 5.987156294 0.297385909 0.767643589 1994 4.88044416 0.594986258 0.904218188 1995 4.651826651 0.617479056 1.209693103 1996 4.725334152 0.860208566 1.424065573 1997 4.435577102 0.625285966 2.329849229 1998 3.74900388 0.472644846 3.244314135 1999 3.475082246 0.765212649 3.405318336 2000 3.51300212 1.056378305 1.630116343 2001 3.609099885 1.011571805 1.015460357 2002 3.48740331 0.605889255 1.022023979 2003 3.483641114 0.765601405 1.749187307 2004 4.554254034 0.88610072 1.091876415 2005 4.508579016 2.130168425 2.160487842 2006 4.40096483 2.073395746 2.398397384 2007 3.73363489 3.620521897 2.318328109 2008 2.568888291 2.651593127 1.114843565 2009 4.0035629 1.635034274 1.761193112 2010 3.708828902 2.002065552 1.695323307 2011 2.827090556 1.312934337 1.545625032 2012 3.039875469 1.516275965 1.716613576 2013 2.559233447 1.695658786 1.436946698 2014 2.192181603 2.092115758 2.80417574 2015 1.55564215 1.937363198 2.53073398 2016 1.349... [truncated]\\n\\nNote: Additional web information can be provided if needed.\\n\\nNote: The following information was retrieved internally from the FAISS index. Please clearly indicate if any part of the answer is based on external web searches.\"}, {'role': 'user', 'content': 'What is KMP algorithm?'}], 'model': 'mixtral-8x7b-32768', 'max_tokens': 200, 'n': 1, 'stop': None, 'stream': False, 'temperature': 0.7}}\n",
      "DEBUG:groq._base_client:Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions\n",
      "DEBUG:httpcore.connection:close.started\n",
      "DEBUG:httpcore.connection:close.complete\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3690f16d0>\n",
      "DEBUG:httpcore.connection:start_tls.started ssl_context=<ssl.SSLContext object at 0x367169890> server_hostname='api.groq.com' timeout=None\n",
      "DEBUG:httpcore.connection:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x3690f1b50>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 08 Nov 2024 21:27:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'5000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'4328'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'8.064s'), (b'x-request-id', b'req_01jc6tsydvfbfr492d93pvgd9a'), (b'via', b'1.1 google'), (b'alt-svc', b'h3=\":443\"; ma=86400'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8df8a5fc38316450-SJC'), (b'Content-Encoding', b'gzip')])\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "DEBUG:groq._base_client:HTTP Response: POST https://api.groq.com/openai/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 08 Nov 2024 21:27:40 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '5000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '4328', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '8.064s', 'x-request-id': 'req_01jc6tsydvfbfr492d93pvgd9a', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=86400', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '8df8a5fc38316450-SJC', 'content-encoding': 'gzip'})\n",
      "INFO:root:The answer is based on information retrieved from the FAISS index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The KMP (Knuth-Morris-Pratt) algorithm is a string matching algorithm that '\n",
      " 'searches for a pattern within a text in a more efficient way than a '\n",
      " 'brute-force approach. It utilizes a preprocessed pattern to skip certain '\n",
      " 'comparisons, achieving a time complexity of O(n + m) where n is the length '\n",
      " 'of the text and m is the length of the pattern. The KMP algorithm is useful '\n",
      " 'in various applications, such as text editing, searching for specific '\n",
      " 'patterns in DNA sequences, and more.\\n'\n",
      " '\\n'\n",
      " 'The KMP algorithm works by creating a prefix function, which calculates the '\n",
      " 'length of the longest proper prefix of the pattern that is also a suffix of '\n",
      " 'the pattern. During the search process, when a mismatch occurs, the '\n",
      " 'algorithm uses the information from the prefix function to skip characters '\n",
      " 'in the text instead of starting the comparison from the beginning of the '\n",
      " 'pattern.\\n'\n",
      " '\\n'\n",
      " 'The KMP algorithm consists of two main steps:\\n'\n",
      " '\\n'\n",
      " '1.')\n",
      "The KMP (Knuth-Morris-Pratt) algorithm is a string matching algorithm that searches for a pattern within a text in a more efficient way than a brute-force approach. It utilizes a preprocessed pattern to skip certain comparisons, achieving a time complexity of O(n + m) where n is the length of the text and m is the length of the pattern. The KMP algorithm is useful in various applications, such as text editing, searching for specific patterns in DNA sequences, and more.\n",
      "\n",
      "The KMP algorithm works by creating a prefix function, which calculates the length of the longest proper prefix of the pattern that is also a suffix of the pattern. During the search process, when a mismatch occurs, the algorithm uses the information from the prefix function to skip characters in the text instead of starting the comparison from the beginning of the pattern.\n",
      "\n",
      "The KMP algorithm consists of two main steps:\n",
      "\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "# Example query to test the system\n",
    "query = \"What is KMP algorithm?\"\n",
    "response = query_system(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
